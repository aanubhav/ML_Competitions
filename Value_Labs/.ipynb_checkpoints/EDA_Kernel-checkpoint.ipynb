{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/value-labs/DataSet/Sample_Submission.csv\n",
      "/kaggle/input/value-labs/DataSet/Test.csv\n",
      "/kaggle/input/value-labs/DataSet/Results.csv\n",
      "/kaggle/input/value-labs/DataSet/Train.csv\n",
      "/kaggle/input/value-labs/DataSet/readme.md\n",
      "/kaggle/input/lm-utils/Language_Model.csv\n",
      "/kaggle/input/lm-utils/transformer-freezed.pth\n",
      "/kaggle/input/lm-utils/lm-fine-tuned-51.pth\n",
      "/kaggle/input/lm-utils/submission(1).csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import re\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.option_context.max_cols = 100\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/value-labs/DataSet/Train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>distractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Meals can be served</td>\n",
       "      <td>in rooms at 9:00 p. m.</td>\n",
       "      <td>'outside the room at 3:00 p. m.', 'in the dining - room at 6:00 p. m.', 'in the dining - room from 7:30 a. m. to 9:15 p. m.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>It can be inferred from the passage that</td>\n",
       "      <td>The local government can deal with the problem of lacking money by some means .</td>\n",
       "      <td>'If some tragedies occur again ', ' relevant departments of the State Council should take responsibility completely .', 'Currently ', ' the central government has established sound management systems to guarantee school bus safety .', 'Central and local governments will share the costs on building more modern schools in rural areas .'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The author called Tommy 's parents in order to</td>\n",
       "      <td>help them realize their influence on Tommy</td>\n",
       "      <td>'blame Tommy for his failing grades', 'blame Tommy for his failing grades'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>It can be inferred from the passage that</td>\n",
       "      <td>the writer is not very willing to use idioms</td>\n",
       "      <td>'idioms are the most important part in a language', 'nonnative speakers should learn more idioms', 'there are no ways to master idioms'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>How can we deal with snake wounds according to the passage ?</td>\n",
       "      <td>Stay calm and do n't move .</td>\n",
       "      <td>'Cut the wound and suck the poison out .'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       question  \\\n",
       "0  Meals can be served                                            \n",
       "1  It can be inferred from the passage that                       \n",
       "2  The author called Tommy 's parents in order to                 \n",
       "3  It can be inferred from the passage that                       \n",
       "4  How can we deal with snake wounds according to the passage ?   \n",
       "\n",
       "                                                                       answer_text  \\\n",
       "0  in rooms at 9:00 p. m.                                                            \n",
       "1  The local government can deal with the problem of lacking money by some means .   \n",
       "2  help them realize their influence on Tommy                                        \n",
       "3  the writer is not very willing to use idioms                                      \n",
       "4  Stay calm and do n't move .                                                       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                         distractor  \n",
       "0  'outside the room at 3:00 p. m.', 'in the dining - room at 6:00 p. m.', 'in the dining - room from 7:30 a. m. to 9:15 p. m.'                                                                                                                                                                                                                      \n",
       "1  'If some tragedies occur again ', ' relevant departments of the State Council should take responsibility completely .', 'Currently ', ' the central government has established sound management systems to guarantee school bus safety .', 'Central and local governments will share the costs on building more modern schools in rural areas .'  \n",
       "2  'blame Tommy for his failing grades', 'blame Tommy for his failing grades'                                                                                                                                                                                                                                                                        \n",
       "3  'idioms are the most important part in a language', 'nonnative speakers should learn more idioms', 'there are no ways to master idioms'                                                                                                                                                                                                           \n",
       "4  'Cut the wound and suck the poison out .'                                                                                                                                                                                                                                                                                                         "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lm_data(train_df):\n",
    "    '''\n",
    "    makes new series for training of language model. This method merges each question:\n",
    "        1) with the answer text\n",
    "        2) with each of the distractor text.\n",
    "    \n",
    "    Eg: question : \"Meals can be served\"\n",
    "        answer_text : \"in rooms at 9:00 p. m.\"\n",
    "        3 distractors : 'outside the room at 3:00 p. m.', 'in the dining - room at 6:00 p. m.', 'in the dining - room from 7:30 a. m. to 9:15 p. m.'\n",
    "        \n",
    "        returns a dataframe with 4 rows : \n",
    "         1) Meals can be served in rooms at 9:00 p. m.\n",
    "         2) Meals can be served outside the room at 3:00 p. m.\n",
    "         3) Meals can be served in the dining - room at 6:00 p. m.\n",
    "         4) Meals can be served in the dining - room from 7:30 a. m. to 9:15 p. m.\n",
    "        \n",
    "    '''\n",
    "    new_series = []\n",
    "    for i in range(len(train_df)):\n",
    "    #     print(train_df.loc[i]['distractor'].split(','), len(train_df.loc[i]['distractor'].split(',')))\n",
    "        for spl in train_df.loc[i]['distractor'].split(','):\n",
    "            distractor = re.sub(\"^([\\s*]*'*\\\"*[\\s*]*)\",\"\",spl)[:-1]\n",
    "    #         print(distractor)\n",
    "            new_series.append(train_df.loc[i]['question']+\" \"+distractor)\n",
    "    merged_df = train_df['question']+\" \"+train_df['answer_text']\n",
    "    merged_df = pd.DataFrame({'merged' : merged_df})\n",
    "    temp_df = pd.DataFrame({'merged' : new_series})\n",
    "    lm_df = pd.concat([merged_df, temp_df])\n",
    "    lm_df.reset_index(drop=True, inplace=True)\n",
    "    return lm_df, new_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_df, _ = make_lm_data(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_df.to_csv('./Language_Model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the notebook was executed in kaggle kernels, had to download data before exiting the session.\n",
    "FileLinks('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the pre-processed data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_df = pd.read_csv(\"../input/lm-utils/Language_Model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Meals can be served in rooms at 9:00 p. m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>It can be inferred from the passage that The local government can deal with the problem of lacking money by some means .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The author called Tommy 's parents in order to help them realize their influence on Tommy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>It can be inferred from the passage that the writer is not very willing to use idioms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>How can we deal with snake wounds according to the passage ? Stay calm and do n't move .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102290</td>\n",
       "      <td>According to Galbraith , people feel discontented because they are in fear of another Great Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102291</td>\n",
       "      <td>According to Galbraith , people feel discontented because public spending has n't been cut down as expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102292</td>\n",
       "      <td>The boy grew up with the smell of the herbs because he liked go play with herbs .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102293</td>\n",
       "      <td>The boy grew up with the smell of the herbs because he studied at a herbal medicine store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102294</td>\n",
       "      <td>The boy grew up with the smell of the herbs because his family had a herbal medicine store</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102295 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                          merged\n",
       "0       Meals can be served in rooms at 9:00 p. m.                                                                              \n",
       "1       It can be inferred from the passage that The local government can deal with the problem of lacking money by some means .\n",
       "2       The author called Tommy 's parents in order to help them realize their influence on Tommy                               \n",
       "3       It can be inferred from the passage that the writer is not very willing to use idioms                                   \n",
       "4       How can we deal with snake wounds according to the passage ? Stay calm and do n't move .                                \n",
       "...                                                                                          ...                                \n",
       "102290  According to Galbraith , people feel discontented because they are in fear of another Great Depression                  \n",
       "102291  According to Galbraith , people feel discontented because public spending has n't been cut down as expected             \n",
       "102292  The boy grew up with the smell of the herbs because he liked go play with herbs .                                       \n",
       "102293  The boy grew up with the smell of the herbs because he studied at a herbal medicine store                               \n",
       "102294  The boy grew up with the smell of the herbs because his family had a herbal medicine store                              \n",
       "\n",
       "[102295 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using [fastai](https://docs.fast.ai/) to build a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import SaveModelCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../input/lm-utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_itos = pickle.load(open(Config().model_path()/'wt103-1/itos_wt103.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to databunch\n",
    "data_lm = TextList.from_df(lm_df, path=path).split_by_rand_pct(0.1, seed=42).label_for_lm().databunch(bs=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.data.TextLMDataBunch"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14712"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_lm.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_lm.vocab.stoi['oct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using [Ulmfit](https://docs.fast.ai/text.learner.html#language_model_learner) for training the Language Model, Which internally uses [AWD_LSTMS](https://arxiv.org/abs/1708.02182)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model_dir='/tmp/.fastai/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm-fine-tuned-51.pth  transformer-freezed.pth  \u001b[0m\u001b[01;34mwt103-fwd\u001b[0m/\r\n",
      "\u001b[01;34mtransformer\u001b[0m/          transformer.tgz          wt103-fwd.tgz\r\n"
     ]
    }
   ],
   "source": [
    "ls /tmp/.fastai/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cp ../input/lm-utils/lm-fine-tuned-51.pth /tmp/.fastai/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lm.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (92066 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj meals can be served in rooms at 9:00 p. m.,xxbos xxmaj it can be inferred from the passage that xxmaj the local government can deal with the problem of lacking money by some means .,xxbos xxmaj the author called xxmaj tommy 's parents in order to help them realize their influence on xxmaj tommy,xxbos xxmaj it can be inferred from the passage that the writer is not very willing to use idioms,xxbos xxmaj how can we deal with snake wounds according to the passage ? xxmaj stay calm and do n't move .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: ../input/lm-utils;\n",
       "\n",
       "Valid: LabelList (10229 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj this year , the xxmaj chinese government make a new environment protection law,xxbos xxmaj the best title of this passage can be xxmaj the pressure of the air,xxbos xxmaj if you really want to meet someone again , it seems necessary for you to let him know it,xxbos xxmaj in the author 's opinion , today 's most important advance in technology lies in brain cells,xxbos xxmaj the author gives us many ideas in order to let us find out if contacts with nature improve our outlook on life\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: ../input/lm-utils;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(14712, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(14712, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=14712, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f95e96eba60>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('../input/lm-utils'), model_dir='/tmp/.fastai/models/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (92066 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj meals can be served in rooms at 9:00 p. m.,xxbos xxmaj it can be inferred from the passage that xxmaj the local government can deal with the problem of lacking money by some means .,xxbos xxmaj the author called xxmaj tommy 's parents in order to help them realize their influence on xxmaj tommy,xxbos xxmaj it can be inferred from the passage that the writer is not very willing to use idioms,xxbos xxmaj how can we deal with snake wounds according to the passage ? xxmaj stay calm and do n't move .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: ../input/lm-utils;\n",
       "\n",
       "Valid: LabelList (10229 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj this year , the xxmaj chinese government make a new environment protection law,xxbos xxmaj the best title of this passage can be xxmaj the pressure of the air,xxbos xxmaj if you really want to meet someone again , it seems necessary for you to let him know it,xxbos xxmaj in the author 's opinion , today 's most important advance in technology lies in brain cells,xxbos xxmaj the author gives us many ideas in order to let us find out if contacts with nature improve our outlook on life\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: ../input/lm-utils;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(14712, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(14712, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=14712, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f95e96eba60>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('../input/lm-utils'), model_dir='/tmp/.fastai/models/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Embedding(14712, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(14712, 400, padding_idx=1)\n",
       "  )\n",
       "  (2): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=14712, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Embedding(14712, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(14712, 400, padding_idx=1)\n",
       "  )\n",
       "  (2): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=14712, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('lm-fine-tuned-51')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fit-head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, slice(1e-07,1e-3), moms=(0.8,0.7), callbacks=[SaveModelCallback(learn, every='epoch', monitor='accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf /tmp/.fastai/models/lm-fine-tuned.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('lm-fine-tuned-51')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /tmp/.fastai/models/lm-fine-tuned-51.pth ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLinks('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? learn.show_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Results csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.read_csv(\"../input/value-labs/DataSet/Results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>distractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>What 'S the main idea of the text ?</td>\n",
       "      <td>The lack of career -- based courses in US high schools</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>In the summer high season , Finland does nt seem to sleep because</td>\n",
       "      <td>the sun is out at night</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>If you want to apply for Chinese Business Internship program successfully , you 'll</td>\n",
       "      <td>have to get confirmed at least twice</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>That afternoon , the boy 's clothes were dry because</td>\n",
       "      <td>nobody made room for him in the water .</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Which of the following statements is NOT true ?</td>\n",
       "      <td>There are twelve countries in the World Wildlife Fund .</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13495</td>\n",
       "      <td>Who helped set up the first company in America ?</td>\n",
       "      <td>Benjamin Franklin .</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13496</td>\n",
       "      <td>Which of the following statements is True according to the passage ?</td>\n",
       "      <td>None of the soldiers who got on the train for the front line had been trained in advance .</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13497</td>\n",
       "      <td>What is the main idea ?</td>\n",
       "      <td>Every person should exercise .</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13498</td>\n",
       "      <td>From the passage we know that</td>\n",
       "      <td>not everybody can buy such a fridge</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13499</td>\n",
       "      <td>What does the circus think of its performing animals ?</td>\n",
       "      <td>They are helpful in saving their species .</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  question  \\\n",
       "0      What 'S the main idea of the text ?                                                   \n",
       "1      In the summer high season , Finland does nt seem to sleep because                     \n",
       "2      If you want to apply for Chinese Business Internship program successfully , you 'll   \n",
       "3      That afternoon , the boy 's clothes were dry because                                  \n",
       "4      Which of the following statements is NOT true ?                                       \n",
       "...                                                ...                                       \n",
       "13495  Who helped set up the first company in America ?                                      \n",
       "13496  Which of the following statements is True according to the passage ?                  \n",
       "13497  What is the main idea ?                                                               \n",
       "13498  From the passage we know that                                                         \n",
       "13499  What does the circus think of its performing animals ?                                \n",
       "\n",
       "                                                                                      answer_text  \\\n",
       "0      The lack of career -- based courses in US high schools                                       \n",
       "1      the sun is out at night                                                                      \n",
       "2      have to get confirmed at least twice                                                         \n",
       "3      nobody made room for him in the water .                                                      \n",
       "4      There are twelve countries in the World Wildlife Fund .                                      \n",
       "...                                                        ...                                      \n",
       "13495  Benjamin Franklin .                                                                          \n",
       "13496  None of the soldiers who got on the train for the front line had been trained in advance .   \n",
       "13497  Every person should exercise .                                                               \n",
       "13498  not everybody can buy such a fridge                                                          \n",
       "13499  They are helpful in saving their species .                                                   \n",
       "\n",
       "       distractor  \n",
       "0     NaN          \n",
       "1     NaN          \n",
       "2     NaN          \n",
       "3     NaN          \n",
       "4     NaN          \n",
       "...    ..          \n",
       "13495 NaN          \n",
       "13496 NaN          \n",
       "13497 NaN          \n",
       "13498 NaN          \n",
       "13499 NaN          \n",
       "\n",
       "[13500 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(learn, df_res):\n",
    "    gen = []\n",
    "    for i in tqdm(range(len(df_res))):\n",
    "        TEXT = df_res.loc[i]['question']\n",
    "        length =len(TEXT)\n",
    "        N_WORDS = 30#len(df_res.loc[i]['answer_text'].split(\" \"))\n",
    "        N_SENTENCES = 3\n",
    "    #     res = [learn.predict(TEXT, N_WORDS, temperature=0.75)[length+1:] for _ in range(N_SENTENCES)]\n",
    "    #     res = [re.split('xxbos', learn.predict(TEXT, N_WORDS, temperature=0.75)[length+1:])[0] for _ in range(N_SENTENCES)]\n",
    "        sentence_string = \"\"\n",
    "        for j in range(N_SENTENCES):\n",
    "            sentence = re.split('xxbos', learn.predict(TEXT, N_WORDS, temperature=0.75)[length+1:])[0]\n",
    "        #      print(sentence)\n",
    "            sentence_string += f\"’{sentence}’, \"\n",
    "        gen.append(sentence_string[:-2])\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#’\n",
    "len(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gen = generate_results(learn=learn, df_res=df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkp = gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_res.drop('distractor', axis=1, inplace=True)\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distractor_df = pd.DataFrame({'distractor':gen})\n",
    "distractor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.concat([df_res, distractor_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLinks(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search for Ulmfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_beam_search(learn, df_res):\n",
    "    gen = []\n",
    "    for i in tqdm(range(len(df_res))):\n",
    "        TEXT = df_res.loc[i]['question']# + ' ' + df_res.loc[i]['answer_text']\n",
    "        length =len(TEXT)\n",
    "        N_WORDS = 30#len(df_res.loc[i]['answer_text'].split(\" \"))\n",
    "        N_SENTENCES = 3\n",
    "        sentence_string = \"\"\n",
    "        for j in range(N_SENTENCES):\n",
    "            sentence = re.split('xxbos', learn.beam_search(TEXT, n_words=N_WORDS, beam_sz=200)[length+1:])[0]\n",
    "            sentence_string += f\"’{sentence}’, \"\n",
    "        gen.append(sentence_string[:-2])\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the summer high season , Finland does nt seem to sleep because Finland is the largest country in the world . xxbos According to the passage , which of the following statements is TRUE according to the passage ?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.beam_search(df_res.loc[1]['question'], n_words=N_WORDS, beam_sz=200)[len(df_res.loc[1]['question'])+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>distractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>What 'S the main idea of the text ?</td>\n",
       "      <td>The lack of career -- based courses in US high schools</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>In the summer high season , Finland does nt seem to sleep because</td>\n",
       "      <td>the sun is out at night</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>If you want to apply for Chinese Business Internship program successfully , you 'll</td>\n",
       "      <td>have to get confirmed at least twice</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>That afternoon , the boy 's clothes were dry because</td>\n",
       "      <td>nobody made room for him in the water .</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Which of the following statements is NOT true ?</td>\n",
       "      <td>There are twelve countries in the World Wildlife Fund .</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              question  \\\n",
       "0  What 'S the main idea of the text ?                                                   \n",
       "1  In the summer high season , Finland does nt seem to sleep because                     \n",
       "2  If you want to apply for Chinese Business Internship program successfully , you 'll   \n",
       "3  That afternoon , the boy 's clothes were dry because                                  \n",
       "4  Which of the following statements is NOT true ?                                       \n",
       "\n",
       "                                               answer_text  distractor  \n",
       "0  The lack of career -- based courses in US high schools  NaN          \n",
       "1  the sun is out at night                                 NaN          \n",
       "2  have to get confirmed at least twice                    NaN          \n",
       "3  nobody made room for him in the water .                 NaN          \n",
       "4  There are twelve countries in the World Wildlife Fund . NaN          "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"’What 'S the main idea of the text ? The United States Conference of Mayors ' Climate Protection Agreement . ’, ’What 'S the main idea of the text ? The United States Conference of Mayors ' Climate Protection Agreement . ’, ’What 'S the main idea of the text ? How to make friends . ’\",\n",
       " '’In the summer high season , Finland does nt seem to sleep because Finland is the largest country in the world . ’, ’In the summer high season , Finland does nt seem to sleep because Finland is the largest country in the world . ’, ’In the summer high season , Finland does nt seem to sleep because it is too far from Finland . ’',\n",
       " \"’If you want to apply for Chinese Business Internship program successfully , you 'll get a Master 's Degree from Harvard University ’, ’If you want to apply for Chinese Business Internship program successfully , you 'll get a Master 's Degree from Master 's College ’, ’If you want to apply for Chinese Business Internship program successfully , you 'll get a Master 's Degree from Harvard University . ’\",\n",
       " \"’That afternoon , the boy 's clothes were dry because he wanted to go to the wise man 's home ’, ’That afternoon , the boy 's clothes were dry because he wanted to wear a pair of shoes ’, ’That afternoon , the boy 's clothes were dry because he wanted to wear a pair of shoes ’\",\n",
       " \"’Which of the following statements is NOT true ? The United Nations declare November 10 Malala Day . ’, ’Which of the following statements is NOT true ? The United Nations declare November 10 Malala Day . ’, ’Which of the following statements is NOT true ? The United States Conference of Mayors ' Climate Protection Agreement is aimed at reducing climate change ’\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_results_beam_search(learn, df_res[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, arch=Transformer, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model_dir='/tmp/.fastai/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ../input/lm-utils/transformer-freezed.pth /tmp/.fastai/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (92066 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj meals can be served in rooms at 9:00 p. m.,xxbos xxmaj it can be inferred from the passage that xxmaj the local government can deal with the problem of lacking money by some means .,xxbos xxmaj the author called xxmaj tommy 's parents in order to help them realize their influence on xxmaj tommy,xxbos xxmaj it can be inferred from the passage that the writer is not very willing to use idioms,xxbos xxmaj how can we deal with snake wounds according to the passage ? xxmaj stay calm and do n't move .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: ../input/lm-utils;\n",
       "\n",
       "Valid: LabelList (10229 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj this year , the xxmaj chinese government make a new environment protection law,xxbos xxmaj the best title of this passage can be xxmaj the pressure of the air,xxbos xxmaj if you really want to meet someone again , it seems necessary for you to let him know it,xxbos xxmaj in the author 's opinion , today 's most important advance in technology lies in brain cells,xxbos xxmaj the author gives us many ideas in order to let us find out if contacts with nature improve our outlook on life\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: ../input/lm-utils;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): Transformer(\n",
       "    (encoder): Embedding(14712, 768)\n",
       "    (pos_enc): Embedding(512, 768)\n",
       "    (drop_emb): Dropout(p=0.03, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=768, out_features=14712, bias=False)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f95e96eba60>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('../input/lm-utils'), model_dir='/tmp/.fastai/models/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (92066 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj meals can be served in rooms at 9:00 p. m.,xxbos xxmaj it can be inferred from the passage that xxmaj the local government can deal with the problem of lacking money by some means .,xxbos xxmaj the author called xxmaj tommy 's parents in order to help them realize their influence on xxmaj tommy,xxbos xxmaj it can be inferred from the passage that the writer is not very willing to use idioms,xxbos xxmaj how can we deal with snake wounds according to the passage ? xxmaj stay calm and do n't move .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: ../input/lm-utils;\n",
       "\n",
       "Valid: LabelList (10229 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj this year , the xxmaj chinese government make a new environment protection law,xxbos xxmaj the best title of this passage can be xxmaj the pressure of the air,xxbos xxmaj if you really want to meet someone again , it seems necessary for you to let him know it,xxbos xxmaj in the author 's opinion , today 's most important advance in technology lies in brain cells,xxbos xxmaj the author gives us many ideas in order to let us find out if contacts with nature improve our outlook on life\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: ../input/lm-utils;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): Transformer(\n",
       "    (encoder): Embedding(14712, 768)\n",
       "    (pos_enc): Embedding(512, 768)\n",
       "    (drop_emb): Dropout(p=0.03, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_att): Dropout(p=0.03, inplace=False)\n",
       "          (drop_res): Dropout(p=0.03, inplace=False)\n",
       "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.03, inplace=False)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=768, out_features=14712, bias=False)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f95e96eba60>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('../input/lm-utils'), model_dir='/tmp/.fastai/models/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Embedding(14712, 768)\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=768, out_features=14712, bias=False)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadAttention(\n",
       "      (attention): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_att): Dropout(p=0.03, inplace=False)\n",
       "      (drop_res): Dropout(p=0.03, inplace=False)\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GeLU()\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.03, inplace=False)\n",
       "        (4): MergeLayer()\n",
       "        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Embedding(14712, 768)\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=768, out_features=14712, bias=False)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('transformer-freezed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, slice(1e-07,1e-3), moms=(0.8,0.7))#, callbacks=[SaveModelCallback(learn, every='epoch', monitor='accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('transformer-freezed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /tmp/.fastai/models/transformer-freezed.pth ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj this year , the xxmaj chinese government make a new environment protection law xxbos xxmaj the best title</td>\n",
       "      <td>of this passage can be xxmaj the pressure of the air xxbos xxmaj if you really want to meet someone</td>\n",
       "      <td>for the passage is be xxmaj the xxmaj of xxmaj xxmaj pollution xxmaj what you want want to be a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxmaj fashion xxbos xxmaj which of the following programs is most suitable for team work ? xxmaj self - led</td>\n",
       "      <td>xxmaj activities xxbos xxmaj the word \" xxunk an example to show that the xxmaj english language is always changing</td>\n",
       "      <td>xxmaj xxunk . xxmaj what passage \" xxunk \" \" of show that the word english words is a a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of money traveling by car xxbos xxmaj the passage is mainly about how to save money while traveling xxbos xxmaj</td>\n",
       "      <td>an \" off- hand speech\"is a speech not longer than three minutes xxbos xxmaj what can we learn from the</td>\n",
       "      <td>what active off- hand speech\"is a speech which a xxbos a xxbos xxbos xxmaj what is we learn from the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj according to the passage , the new treatment can double the number of disease - fighting cells xxbos</td>\n",
       "      <td>xxmaj what is implied in the passage by the writer ? xxmaj to use wealth properly xxbos xxmaj when did</td>\n",
       "      <td>xxmaj what is the in the passage ? the author ? xxmaj the make the and . xxmaj what the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>old woman took a wrong train because she had never taken a train before . xxbos xxmaj what did the</td>\n",
       "      <td>author do besides his studies while in xxmaj london ? xxmaj he made contact with friends from his home .</td>\n",
       "      <td>author think after the career ? he the xxunk ? xxmaj he was a with the . xxmaj father .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
